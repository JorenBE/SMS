{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The goal of this notebook is to make a .pkl file where the abstract, introduction, body, conclusion and the main text and the doi entry are in a dataframe to be used by DataLocationProcess.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pdfplumber\n",
    "import os\n",
    "import fitz\n",
    "import re\n",
    "import requests\n",
    "\n",
    "pdf_location = './data/balasz'\n",
    "csv_location = './data/files/20231030_BAN_shared_fixed.csv'\n",
    "data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to clean illegal characters\n",
    "def clean_illegal_characters(value):\n",
    "    if isinstance(value, str):\n",
    "        # Remove control characters\n",
    "        value = re.sub(r'[\\x00-\\x1F\\x7F-\\x9F]', '', value)\n",
    "    return value\n",
    "\n",
    "def get_doi_metadata(doi):\n",
    "    # CrossRef REST API URL\n",
    "    url = f\"https://api.crossref.org/works/{doi}\"\n",
    "\n",
    "    try:\n",
    "        # Send a GET request to the API\n",
    "        response = requests.get(url)\n",
    "        # Check if the response is successful\n",
    "        if response.status_code == 200:\n",
    "            # Return the JSON metadata\n",
    "            return response.json()['message']\n",
    "        else:\n",
    "            print(f\"Error fetching DOI {doi}: {response.status_code}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching DOI {doi}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "class cleanArticleText:\n",
    "    def __init__(self, text: str):\n",
    "        self.text = text\n",
    "        self.abstract = None  # To store the abstract text\n",
    "\n",
    "    def cleanText(self):\n",
    "        # Regular expressions to locate sections\n",
    "        intro_pattern = re.compile(r'\\bIntroduction\\b', re.IGNORECASE)\n",
    "        abs_pattern = re.compile(r'\\bA\\s*B\\s*S\\s*T\\s*R\\s*A\\s*C\\s*T\\b', re.IGNORECASE)\n",
    "        refs_pattern = re.compile(r'\\bReferences\\b', re.IGNORECASE)\n",
    "        ack_pattern = re.compile(r'\\bAcknowledgements?\\b', re.IGNORECASE)\n",
    "\n",
    "        # Find start of \"Introduction\" and \"Abstract\"\n",
    "        intro_match = intro_pattern.search(self.text)\n",
    "        abs_match = abs_pattern.search(self.text)\n",
    "\n",
    "        # Extract Abstract if it exists\n",
    "        if abs_match:\n",
    "            abs_start = abs_match.end()\n",
    "            abs_end = intro_match.start() if intro_match else None\n",
    "            self.abstract = self.text[abs_start:abs_end].strip()\n",
    "\n",
    "        # Determine the start position based on the first occurrence of Abstract or Introduction\n",
    "        if abs_match and intro_match:\n",
    "            start_position = min(abs_match.end(), intro_match.end())\n",
    "        elif abs_match:\n",
    "            start_position = abs_match.end()\n",
    "        elif intro_match:\n",
    "            start_position = intro_match.end()\n",
    "        else:\n",
    "            # If neither Abstract nor Introduction is found, start from the beginning\n",
    "            start_position = 0\n",
    "\n",
    "        # Find first occurrence of \"References\" or \"Acknowledgements\"\n",
    "        refs_match = refs_pattern.search(self.text)\n",
    "        ack_match = ack_pattern.search(self.text)\n",
    "\n",
    "        # Determine the end position based on the first occurrence of References or Acknowledgements\n",
    "        end_position = None\n",
    "        if refs_match and ack_match:\n",
    "            end_position = min(refs_match.start(), ack_match.start())\n",
    "        elif refs_match:\n",
    "            end_position = refs_match.start()\n",
    "        elif ack_match:\n",
    "            end_position = ack_match.start()\n",
    "\n",
    "        # Handle cases where end_position might be invalid\n",
    "        if end_position and start_position > end_position:\n",
    "            start_position = 0\n",
    "\n",
    "        # Slice the text based on the determined start and end positions\n",
    "        if end_position:\n",
    "            cleaned_text = self.text[start_position:end_position]\n",
    "        else:\n",
    "            cleaned_text = self.text[start_position:]\n",
    "\n",
    "        return cleaned_text.strip(), self.abstract  # Return both the cleaned text and the abstract\n",
    "\n",
    "    def extractSection(self, keywords: str):\n",
    "        # Regular expressions to locate sections\n",
    "        pattern = re.compile(r'\\b{}\\b'.format(keywords), re.IGNORECASE)\n",
    "        match = pattern.search(self.text)\n",
    "        if match:\n",
    "            start_position = match.end()\n",
    "            return self.text[start_position:]\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "\n",
    "def extract_section_between(text, start_pattern, end_pattern):\n",
    "    \"\"\"Extract text between two patterns.\"\"\"\n",
    "    start_match = re.search(start_pattern, text, re.IGNORECASE)\n",
    "    end_match = re.search(end_pattern, text, re.IGNORECASE)\n",
    "\n",
    "    if start_match:\n",
    "        start_index = start_match.end()\n",
    "        end_index = end_match.start() if end_match else len(text)\n",
    "        return text[start_index:end_index].strip()\n",
    "    return \"Section not found\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(csv_location)\n",
    "print('number of unique doi:', len(df['reported'].unique()))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process PDFs\n",
    "for filename in os.listdir(pdf_location):\n",
    "    if filename.endswith(\".pdf\"):\n",
    "        # Extract DOI from filename\n",
    "        downloaded_doi = filename.replace(\".pdf\", \"\")[:-2].replace(\"_\", \"/\")\n",
    "        doi_metadata = get_doi_metadata(downloaded_doi)\n",
    "\n",
    "        # Read PDF content\n",
    "        pdf_path = os.path.join(pdf_location, filename)\n",
    "        doc = fitz.open(pdf_path)\n",
    "        full_text = \"\"\n",
    "        for page in doc:\n",
    "            full_text += page.get_text(\"text\")\n",
    "\n",
    "        # Use the cleanArticleText class to process the text\n",
    "        cleaner = cleanArticleText(full_text)\n",
    "        cleaned_text, abstract = cleaner.cleanText()\n",
    "\n",
    "        # Fallback if abstract is not found\n",
    "        if not abstract:\n",
    "            abstract = \"Abstract not found. Extracted from start of article.\"\n",
    "\n",
    "        # Extract title from DOI metadata\n",
    "        title = doi_metadata.get('title', ['N/A'])[0] if doi_metadata else 'N/A'\n",
    "\n",
    "        # Append to the data list\n",
    "        data.append({\n",
    "            \"Filename\": filename,\n",
    "            \"Title\": title.strip(),\n",
    "            \"DOI\": downloaded_doi,\n",
    "            \"Abstract\": abstract.strip(),\n",
    "            \"Main Content\": cleaned_text.strip()\n",
    "        })\n",
    "\n",
    "# Convert the data to a DataFrame\n",
    "articles_df = pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downloaded_doi = []\n",
    "for filename in os.listdir(pdf_location):\n",
    "    if filename.endswith(\".pdf\"):\n",
    "        downloaded_doi.append(filename.replace(\".pdf\", \"\")[:-2].replace(\"_\", \"/\"))\n",
    "\n",
    "downloaded_set = set(downloaded_doi)\n",
    "reported_set = set(df['reported'].unique())\n",
    "undownloaded_dois = reported_set - downloaded_set\n",
    "print('Undownloaded DOIs:', undownloaded_dois)\n",
    "\n",
    "# Find any DOIs in downloaded_set that are not in reported_set\n",
    "missing_in_reported = downloaded_set - reported_set\n",
    "\n",
    "# Check if there are any missing DOIs\n",
    "if missing_in_reported:\n",
    "    print('DOIs in downloaded set but not in reported set:', missing_in_reported)\n",
    "else:\n",
    "    print('All downloaded DOIs are in the reported set.')\n",
    "\n",
    "duplicates = [doi for doi in downloaded_doi if downloaded_doi.count(doi) > 1]\n",
    "# Print the duplicates\n",
    "if duplicates:\n",
    "    print('Duplicates in downloaded DOIs:', set(duplicates))\n",
    "else:\n",
    "    print('No duplicates in downloaded DOIs.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_df = articles_df.applymap(clean_illegal_characters)\n",
    "articles_df.to_excel(\"processed_articles.xlsx\", index=False, engine=\"openpyxl\")\n",
    "articles_df.to_csv(\"processed_articles.csv\", index=False, encoding=\"utf-8\", sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjusted start pattern to include symbols and whitespace around INTRODUCTION\n",
    "articles_df = pd.read_pickle(\"processed_articles.pkl\")\n",
    "i = 69\n",
    "# columnHeader = 'Abstract'\n",
    "# columnHeader = 'Introduction'\n",
    "columnHeader = 'Conclusion'\n",
    "start_pattern = r'Conclusions'\n",
    "end_pattern = r'Conï¬‚icts of interest' \n",
    "text_to_extract = articles_df['Main Content'][i]\n",
    "# print('main content: ', articles_df['Main Content'][i])\n",
    "print('abstract: ', articles_df['Abstract'][i])\n",
    "print(articles_df['Filename'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual extraction of abstract, introduction, and conclusion sections\n",
    "def extract_section_between(text, start_pattern, end_pattern):\n",
    "    \"\"\"Extract text between two patterns.\"\"\"\n",
    "    start_match = re.search(start_pattern, text, re.IGNORECASE)\n",
    "    end_match = re.search(end_pattern, text, re.IGNORECASE)\n",
    "    print('start_match:', start_match)\n",
    "    print('end_match:', end_match)\n",
    "\n",
    "    if start_match:\n",
    "        start_index = start_match.end()\n",
    "        end_index = end_match.start() if end_match else len(text)\n",
    "        return text[start_index:end_index].strip()\n",
    "    return \"Section not found\"\n",
    "\n",
    "# Call the function\n",
    "temp = extract_section_between(text_to_extract, start_pattern, end_pattern)\n",
    "print('Extracted Section:', temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If pattern matching is not working, try to copy and pasete the text manually in temp\n",
    "# temp = ''''''\n",
    "# articles_df.at[i, columnHeader] = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame as a pickle file\n",
    "articles_df.to_pickle('processed_articles.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pdfExtract",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
